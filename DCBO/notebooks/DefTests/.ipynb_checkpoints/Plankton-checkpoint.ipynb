{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16b8ccb-ae6b-41d6-962d-80c22221c9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"../../src/\")\n",
    "sys.path.append(\"../../data/\")\n",
    "sys.path.append(\"../..\")\n",
    "\n",
    "from src.experimental.experiments import create_plankton_dataset\n",
    "from src.experimental.experiments import optimal_sequence_of_interventions\n",
    "from src.utils.sem_utils.real_sems import PredatorPreySEM as PPSEM\n",
    "from src.utils.sem_utils.sem_estimate import build_sem_hat\n",
    "from src.utils.sequential_intervention_functions import get_interventional_grids\n",
    "\n",
    "import pygraphviz\n",
    "from networkx.drawing import nx_agraph\n",
    "from src.utils.dag_utils.graph_functions import make_graphical_model\n",
    "from src.utils.utilities import powerset\n",
    "\n",
    "from src.experimental.experiments import run_methods_replicates\n",
    "from src.experimental.analyse_results import get_relevant_results, elaborate\n",
    "from src.utils.plotting import plot_expected_opt_curve_paper\n",
    "\n",
    "from src.methods.bo import BO\n",
    "from src.methods.dcbo import DCBO\n",
    "from src.methods.pibo import PIBO\n",
    "from src.methods.dcpibo import DCPIBO\n",
    "\n",
    "from matplotlib.pyplot import rc\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import random\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c2a0a7-1d20-49d4-b5c0-769c2c9f3730",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b83fe8c-446b-43be-aa32-56bccabc4f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_list(input_list, exponent):\n",
    "    '''Raises each element of the given input_list to the desired exponent'''\n",
    "    \n",
    "    return_list = []\n",
    "    \n",
    "    for element in input_list:\n",
    "        if element >= 0:\n",
    "            raised_element = element**exponent\n",
    "        else:\n",
    "            raised_element = -(abs(element)**exponent)\n",
    "        return_list.append(raised_element)\n",
    "        \n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54cf8bd6-6fe1-4d81-a09d-9cb1c0830072",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_statistics(y_function, standard_deviation):\n",
    "    '''Computes all the necessary lists associated to the Normal Distribution'''\n",
    "    \n",
    "    return_variates = []\n",
    "    return_amps = []\n",
    "    return_pdfs = []\n",
    "    \n",
    "    for mean in y_function:\n",
    "        return_variates.append(stats.norm.rvs(mean, standard_deviation, 10))\n",
    "        \n",
    "        amp = np.linspace(mean-5*standard_deviation, mean+5*standard_deviation, 10)\n",
    "        return_amps.append(amp)\n",
    "        \n",
    "        return_pdfs.append(stats.norm.pdf(amp, mean, standard_deviation))\n",
    "    \n",
    "    return return_variates, return_amps, return_pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52469750-25c9-45c5-a633-ea42b916e044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise_pdfs(pdfs_list):\n",
    "    '''Normalises the PDFs between 0 and 1'''\n",
    "    \n",
    "    return_normalised_pdfs_list = []\n",
    "    \n",
    "    for pdf_list in pdfs_list:\n",
    "        temp_list = []\n",
    "        \n",
    "        pdf_min = min(pdf_list)\n",
    "        pdf_max = max(pdf_list)\n",
    "        \n",
    "        for pdf_value in pdf_list:\n",
    "            temp_list.append(round((pdf_value-pdf_min)/(pdf_max-pdf_min),2))\n",
    "        \n",
    "        return_normalised_pdfs_list.append(temp_list)\n",
    "        \n",
    "    return return_normalised_pdfs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f3e85e2-b32b-4ef5-a274-223a872496af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regret_priors(start_prior):\n",
    "    '''Adds regret'''\n",
    "    \n",
    "    return_raised_priors = []\n",
    "    \n",
    "    for i in range(1,N+1):\n",
    "        temp = []\n",
    "        gamma = beta/i\n",
    "        \n",
    "        for p_list in start_prior:\n",
    "            temp.append(power_list(p_list, gamma))\n",
    "            \n",
    "        return_raised_priors.append(temp)\n",
    "    \n",
    "    return return_raised_priors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5daac13-ae37-4a50-8322-119cad674ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_optima(time_steps, variates, normalised_pdfs):\n",
    "    '''Predicts the optimum for each time-step'''\n",
    "    \n",
    "    return_prediction = []\n",
    "    \n",
    "    for time_step in range(time_steps):\n",
    "        if(min(variates[time_step])+max(variates[time_step]))<0:\n",
    "            optimum = min(variates[time_step]*normalised_pdfs[time_step])\n",
    "        else:\n",
    "            optimum = max(variates[time_step]*normalised_pdfs[time_step])\n",
    "        return_prediction.append(optimum)\n",
    "        \n",
    "    return return_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19501193-c732-4710-aee4-563ea319baef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_optima_regret(iterations, time_steps, regret_priors, normalised_pdfs):\n",
    "    '''Computes the prediction for each time-step and each iteration, according to the effect of gamma (regret)'''\n",
    "    \n",
    "    return_predictions = []\n",
    "    \n",
    "    for iteration in range(iterations):\n",
    "        temp = []\n",
    "        \n",
    "        for time_step in range(time_steps):\n",
    "            if(min(regret_priors[iteration][time_step])+max(regret_priors[iteration][time_step]))<0:\n",
    "                optimum = min(np.multiply(regret_priors[iteration][time_step], normalised_pdfs[time_step]))\n",
    "            else:\n",
    "                optimum = max(np.multiply(regret_priors[iteration][time_step], normalised_pdfs[time_step]))\n",
    "                              \n",
    "            temp.append(optimum)\n",
    "                              \n",
    "        return_predictions.append(temp)\n",
    "                              \n",
    "    return return_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7032c318-63cc-4fb5-96ae-ff279512aae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_plankton_SEM(T):\n",
    "    \n",
    "    p_dict = create_plankton_dataset(1, T)\n",
    "    \n",
    "    P_SEM = PPSEM()\n",
    "    p_stat_sem = PPSEM.static(P_SEM)\n",
    "    p_dyn_sem = PPSEM.dynamic(P_SEM)\n",
    "    \n",
    "    slice_node_set = [\"M\", \"N\", \"P\", \"J\", \"A\", \"E\", \"D\"]\n",
    "    dag_view = make_graphical_model(0, T-1, topology=\"dependent\", nodes=slice_node_set, verbose=True)\n",
    "    G = nx_agraph.from_agraph(pygraphviz.AGraph(dag_view.source))\n",
    "    \n",
    "    for t in range(T-1):\n",
    "        G.add_edge(\"P_{}\".format(t), \"N_{}\".format(t + 1))\n",
    "        G.add_edge(\"A_{}\".format(t), \"J_{}\".format(t + 1))\n",
    "        G.remove_edge(\"M_{}\".format(t), \"M_{}\".format(t+1))\n",
    "        \n",
    "    for t in range(T):\n",
    "        G.remove_edge(\"J_{}\".format(t), \"A_{}\".format(t))\n",
    "        G.add_edge(\"P_{}\".format(t), \"A_{}\".format(t))\n",
    "        G.add_edge(\"P_{}\".format(t), \"E_{}\".format(t))\n",
    "        G.add_edge(\"J_{}\".format(t), \"D_{}\".format(t))\n",
    "        G.add_edge(\"A_{}\".format(t), \"D_{}\".format(t))\n",
    "        \n",
    "    #  Specifiy all the exploration sets based on the manipulative variables in the DAG\n",
    "    exploration_sets = list(powerset([\"M\", \"J\", \"A\"]))\n",
    "    # Specify the intervention domain for each variable\n",
    "    intervention_domain = {\"M\": [40.0, 160.0], \"J\": [0.0, 20.0], \"A\":[0.0, 100.0]}\n",
    "    # Specify a grid over each exploration and use the grid to find the best intevention value for that ES\n",
    "    interventional_grids = get_interventional_grids(exploration_sets, intervention_domain, size_intervention_grid=100)\n",
    "    \n",
    "    _, optimal_interventions, true_objective_values, _, _, all_causal_effects = optimal_sequence_of_interventions(\n",
    "        exploration_sets=exploration_sets,\n",
    "        interventional_grids=interventional_grids,\n",
    "        initial_structural_equation_model=p_stat_sem,\n",
    "        structural_equation_model=p_dyn_sem,\n",
    "        G=G,\n",
    "        T=T,\n",
    "        model_variables=slice_node_set,\n",
    "        target_variable=\"D\",\n",
    "    )\n",
    "    \n",
    "    return (\n",
    "        p_stat_sem,\n",
    "        p_dyn_sem,\n",
    "        dag_view,\n",
    "        G,\n",
    "        exploration_sets,\n",
    "        intervention_domain,\n",
    "        true_objective_values,\n",
    "        optimal_interventions,\n",
    "        all_causal_effects,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fdcac1-e01f-479a-99d1-4b308ee7511a",
   "metadata": {},
   "source": [
    "## Constants and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c750a4d3-0b0d-4b40-97cb-a345abe07efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 4\n",
    "N = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b486cdb9-1035-4392-a9c1-3156340c2701",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d501de91-e4ef-40da-a024-97fb6a4040d0",
   "metadata": {},
   "source": [
    "Concerning the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f6c2a54c-e7d2-45e0-b4db-8d58216ef8e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " /mnt/c/Users/lucal/Desktop/Thesis/DCBO/notebooks/DefTests/../../src/utils/sem_utils/real_sems.py:55: RuntimeWarning:invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "init_sem, sem, dag_view, G, exploration_sets, intervention_domain, true_objective_values, optimal_interventions, all_causal_effects = setup_plankton_SEM(T=T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1c174213-adb9-4889-8313-20bd8dbd1069",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'predicted_optima_regret' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m R \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m run_methods_replicates(G\u001b[38;5;241m=\u001b[39mG, \n\u001b[1;32m      3\u001b[0m                                 sem\u001b[38;5;241m=\u001b[39mPPSEM, \n\u001b[1;32m      4\u001b[0m                                 make_sem_estimator\u001b[38;5;241m=\u001b[39mbuild_sem_hat, \n\u001b[1;32m      5\u001b[0m                                 base_target_variable\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      6\u001b[0m                                 intervention_domain \u001b[38;5;241m=\u001b[39m intervention_domain, \n\u001b[1;32m      7\u001b[0m                                 methods_list \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBO\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      8\u001b[0m                                 obs_samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m                                 exploration_sets \u001b[38;5;241m=\u001b[39m exploration_sets,\n\u001b[0;32m---> 10\u001b[0m                                 priors_regret \u001b[38;5;241m=\u001b[39m \u001b[43mpredicted_optima_regret\u001b[49m,\n\u001b[1;32m     11\u001b[0m                                 total_timesteps \u001b[38;5;241m=\u001b[39m T,\n\u001b[1;32m     12\u001b[0m                                 number_of_trials \u001b[38;5;241m=\u001b[39m N, \n\u001b[1;32m     13\u001b[0m                                 reps \u001b[38;5;241m=\u001b[39m R, \u001b[38;5;66;03m# Number of replicates (how many times we run each method)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m                                 n_restart \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     15\u001b[0m                                 save_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m                                 n_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, \u001b[38;5;66;03m# The method samples 5 time-series for each replicate\u001b[39;00m\n\u001b[1;32m     17\u001b[0m                                 num_anchor_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m     18\u001b[0m                                 sample_anchor_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     19\u001b[0m                                 controlled_experiment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'predicted_optima_regret' is not defined"
     ]
    }
   ],
   "source": [
    "R = 1\n",
    "results = run_methods_replicates(G=G, \n",
    "                                sem=PPSEM, \n",
    "                                make_sem_estimator=build_sem_hat, \n",
    "                                base_target_variable='Y',\n",
    "                                intervention_domain = intervention_domain, \n",
    "                                methods_list = ['BO'],\n",
    "                                obs_samples = None,\n",
    "                                exploration_sets = exploration_sets,\n",
    "                                priors_regret = [],\n",
    "                                total_timesteps = T,\n",
    "                                number_of_trials = N, \n",
    "                                reps = R, # Number of replicates (how many times we run each method)\n",
    "                                n_restart = 1,\n",
    "                                save_data = False,\n",
    "                                n_obs = 5, # The method samples 5 time-series for each replicate\n",
    "                                num_anchor_points = 100,\n",
    "                                sample_anchor_points = True,\n",
    "                                controlled_experiment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045c1185-f977-4ef3-b397-4e5810c8d8ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
